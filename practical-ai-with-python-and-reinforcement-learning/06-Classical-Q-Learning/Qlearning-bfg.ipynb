{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "15e3ff29-92ff-4221-97d9-a5b24e4f3018",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3aa5bcb6-f4f2-4ab5-bd1b-90e964aee37e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "64ce01ff-b851-435f-af99-4b4c0fb56a69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "# to get cells to clear so you see an animation\n",
    "\n",
    "# first way\n",
    "import os\n",
    "os.system('clear')\n",
    "\n",
    "#second way.\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2caaae38-d4a3-468d-a7c2-23a74517a59d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already registered\n"
     ]
    }
   ],
   "source": [
    "from gym.envs.registration import register\n",
    "\n",
    "try:\n",
    "    register(\n",
    "        id='FrozenLakeNotSlipery-v0',\n",
    "        entry_point='gym.envs.toy_text:FrozenLakeEnv',\n",
    "        kwargs={'map_name' : '4x4', 'is_slippery': False},  # set the map\n",
    "        max_episode_steps=100,  # number of steps before arbitrary ending.\n",
    "        reward_threshold=0.78,  # (level of award recived before marking it as solved)\n",
    "\n",
    "    )\n",
    "except:\n",
    "        print('Already registered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "176a15a6-89ed-45f6-948c-92b74a49016f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[41mS\u001b[0mFFF\n",
      "FHFH\n",
      "FFFH\n",
      "HFFG\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('FrozenLakeNotSlipery-v0')\n",
    "env.reset()  # reset before replaying\n",
    "\n",
    "for step in range(200):\n",
    "    env.render()  # create the game space\n",
    "    action = env.action_space.sample()  # take a random action from the action sample space.\n",
    "    observation, reward, done, info = env.step(action)  # take a step of action.\n",
    "    time.sleep(0.3)  # So we can watch, not needed because it sleeps\n",
    "    clear_output(wait=True)\n",
    "    if done:\n",
    "        env.reset()\n",
    "        \n",
    "env.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e97e7e05-869d-448c-ac6c-d4d19c5c4f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State Size: 16, Action Size: 4, Q_Table Size:16x4\n"
     ]
    }
   ],
   "source": [
    "# define the number of possible actions.\n",
    "# the action population\n",
    "action_size = env.action_space.n\n",
    "state_size = env.observation_space.n\n",
    "\n",
    "print(f'State Size: {state_size}, Action Size: {action_size}, Q_Table Size:{state_size}x{action_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c40e8d9c-92f0-44f0-a7db-10624b1c4635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rows--Sates columns--Actions\n",
    "\n",
    "q_table = np.zeros([state_size, action_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5c074e6c-441a-434c-90a1-c6187dc3dab3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "q_table  # All q values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ee403be-dad1-4e25-a88b-e379de427ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS=20000  # episodes - how many times the agent plays the game to 'done' status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7371d817-d078-4272-94e4-4d42fb160627",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALPHA = 0.8  # Learning Rate : low=slow, high=may not converge.\n",
    "GAMMA = 0.95 # Discount Rate : close to 1 | gamma^2 r + gamma ^3 ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f95ba6ff-12ef-42cb-90f6-aea3d736dc40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will need three functions for our Q-Table learning process.\n",
    "\n",
    "# Epsilon-GReedy Action Selection\n",
    "# Computation of Next Q Value\n",
    "# Epsilon Reduction\n",
    "\n",
    "\n",
    "# Epsilon-GReedy Action Selection\n",
    "# - Designed to take in epsilon and current state of agent in the env.\n",
    "# - Decided whether to choose the action for the MaxQ or random\n",
    "# - Uses Epsilon in determining this choice\n",
    "# - Return Action (Random or Q-based)\n",
    "\n",
    "# Computation of next Q-value\n",
    "# - Simply a programatic version of the Q-Learning Update Function.\n",
    "# - Q(St,At) <- Q(St, At) ⍺[Rt+1 + Ɣ max(a)Q(St+1,a) - Q(St,At)]\n",
    "\n",
    "# Reduction of Epsilon\n",
    "# - Function designed to reduce epsilon after each epoch/episode.\n",
    "# - As epsilon is reduced the chance of choosing a random action instead of\n",
    "#   an action based on the max Q values is also reduced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "961756fd-debc-4665-9737-5016db851826",
   "metadata": {},
   "outputs": [],
   "source": [
    "# exploration vs exploitation strategy.\n",
    "# Max Q or Max Action\n",
    "\n",
    "epsilon = 1.0  # default exploration rate.\n",
    "max_epsilon = 1\n",
    "min_epsilon = 0.01\n",
    "decay_rate = 0.001\n",
    "\n",
    "# Reduce epsilon over time to use the most successful path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "681d844e-bb45-4c40-926b-d2e706a05f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epsilon_greedy_action_selection(epsilon, q_table, discrete_state):\n",
    "    \n",
    "    random_number = np.random.random()  # returns a number between 0.0 - 1.0\n",
    "    \n",
    "    # Exploitation (choose the action that maximises Q)\n",
    "    if random_number > epsilon:  # initial state 1.0, always random.\n",
    "        \n",
    "        state_row = q_table[discrete_state,:]\n",
    "        action = np.argmax(state_row)\n",
    "    \n",
    "    # Exploration (choose a random action)\n",
    "    else:\n",
    "        action = env.action_space.sample()\n",
    "        \n",
    "    return action\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "19386a26-a623-498c-bf17-4e2d4df9978a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Action 0, 1, 2, 3\n",
    "s_row = np.array([0, 0.1, 0, 0])  # Row state row.\n",
    "# np.argmax on the state row to select action\n",
    "# grab the index position at the max value.\n",
    "# action is the index.\n",
    "\n",
    "np.argmax(s_row) # returns the highest val in the row as an index. (pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "fb87bdfb-7920-499a-8267-1f9501fe2987",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computing the next Q value.\n",
    "def compute_next_q_value(old_q_value, reward, next_optimal_q_value):\n",
    "    \n",
    "    return old_q_value + ALPHA * (reward + GAMMA * next_optimal_q_value - old_q_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "149605f3-f245-4a82-91e6-c2e294857276",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_epsilon(epsilon, epoch):\n",
    "    # return epsilon -= 0.001  # linear approach.\n",
    "    return min_epsilon + (max_epsilon - min_epsilon) * np.exp(-decay_rate * epoch)  # exponetial decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72317699-6904-43d2-b583-275f9367c56d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
